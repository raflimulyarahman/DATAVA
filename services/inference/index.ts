import express, { Request, Response } from 'express';
import axios from 'axios';
import cors from 'cors';

const app = express();
const port = process.env.PORT || 5052;

// Enable CORS for all routes
app.use(cors());

// Parse JSON bodies
app.use(express.json({ limit: '10mb' }));

// Health check endpoint
app.get('/', (req: Request, res: Response) => {
  res.json({ message: 'DATAVA Inference Service', status: 'running' });
});

// Inference endpoint
app.post('/infer', async (req: Request, res: Response) => {
  try {
    const { input, poolId, metadata } = req.body;

    if (!input) {
      return res.status(400).json({ error: 'Input prompt is required' });
    }

    console.log(`Processing inference request for pool: ${poolId}`);

    // Get OpenAI API key from environment
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (!openaiApiKey) {
      console.error('OpenAI API key not found in environment variables');
      return res.status(500).json({ error: 'Server configuration error: API key missing' });
    }

    // Get model and system prompt from environment
    const model = process.env.MODEL || 'gpt-4.1'; // Note: gpt-4.1 may not exist, using for demo
    const systemPrompt = process.env.SYSTEM_PROMPT || 'You are DATAVA cooperative model. Answer concisely.';
    
    // In a real implementation, we would call the OpenAI API
    // For this example, we'll simulate the API call and return a mock response
    // In practice, you would send the request to OpenAI's endpoint
    
    // Simulate API call delay
    await new Promise(resolve => setTimeout(resolve, 2000));
    
    // Mock response (in real implementation, this would come from OpenAI)
    const mockResponse = `This is a simulated response from the model to your input: "${input}". In a real implementation, this would be generated by ${model}. The system prompt was: "${systemPrompt}".`;
    
    console.log(`Inference completed for pool: ${poolId}`);
    
    res.json({ 
      text: mockResponse,
      tokens: mockResponse.split(' ').length,
      model: model,
      poolId: poolId
    });
  } catch (error) {
    console.error('Inference error:', error);
    res.status(500).json({ error: 'Inference failed', details: (error as Error).message });
  }
});

app.listen(port, () => {
  console.log(`DATAVA Inference Service running on port ${port}`);
  console.log(`Model: ${process.env.MODEL || 'gpt-4.1'}`);
  console.log(`System prompt: ${process.env.SYSTEM_PROMPT || 'You are DATAVA cooperative model. Answer concisely.'}`);
});